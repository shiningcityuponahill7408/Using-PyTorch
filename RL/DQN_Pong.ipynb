{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install gym[atari,accept-rom-license]"
      ],
      "metadata": {
        "id": "N13KZNizQJLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIkCIrmOQDAc"
      },
      "outputs": [],
      "source": [
        "%%writefile dqn_pong.py\n",
        "\n",
        "import wrappers\n",
        "import dqn_model\n",
        "\n",
        "import argparse\n",
        "import time\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "DEFAULT_ENV_NAME = \"PongNoFrameskip-v4\"\n",
        "MEAN_REWARD_BOUND = 19\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.01\n",
        "\n",
        "Experience = collections.namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"done\", \"new_sate\"])\n",
        "\n",
        "class ExperienceBuffer():\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = collections.deque(maxlen = capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    indices = np.random.choice(len(self.buffer), batch_size, replace = False)\n",
        "\n",
        "    states, actions, rewards, dones, next_states = \\\n",
        "    zip(*[self.buffer[index] for index in indices])\n",
        "\n",
        "    return np.array(states), np.array(actions), \\\n",
        "               np.array(rewards, dtype=np.float32), \\\n",
        "               np.array(dones, dtype=np.uint8), \\\n",
        "               np.array(next_states)\n",
        "\n",
        "\n",
        "class Agent():\n",
        "  def __init__(self, env, exp_buffer):\n",
        "    self.env = env\n",
        "    self.exp_buffer = exp_buffer\n",
        "    self._reset()\n",
        "\n",
        "  def _reset(self):\n",
        "    self.state = env.reset()\n",
        "    self.total_reward = 0.0\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_step(self, net, epsilon = 0.0, device = \"cpu\"):\n",
        "    done_reward = None\n",
        "\n",
        "    \n",
        "    if np.random.random() < epsilon:\n",
        "      # choose random action according to epsilon prob\n",
        "      # epsilon will decay from 1.0 to 0.01 in 150000 episode linearly\n",
        "      action = self.env.action_space.sample()\n",
        "\n",
        "    else:\n",
        "      state_a = np.array([self.state], copy = False)\n",
        "      state_v = torch.tensor(state_a).to(device)\n",
        "      q_vals_v = net(state_v)\n",
        "      _, act_v = torch.max(q_vals_v, dim = 1)\n",
        "      action = act_v.item()\n",
        "\n",
        "      # take a step in the env\n",
        "\n",
        "      new_state, reward, is_done = env.step(action)\n",
        "      self.total_reward += reward\n",
        "\n",
        "      exp = Experience(self.state, action, reward, is_done, new_state)\n",
        "\n",
        "      self.exp_buffer.append(exp)\n",
        "\n",
        "      self.state = new_state\n",
        "\n",
        "      if is_done:\n",
        "        done_reward = total_reward\n",
        "        self._reset()\n",
        "\n",
        "      return done_reward\n",
        " \n",
        "  def calc_loss(batch, net, tgt_net, device = \"cpu\"):\n",
        "    # get batch of random experiences\n",
        "    states, actions, rewards, dones, next_states = batch\n",
        "\n",
        "    # convert all of them to torch tensor  \n",
        "    states_v = torch.tensor(np.array(states, copy = False)).to(device)\n",
        "    actions_v = torch.tensor(actions).to(device)\n",
        "    next_states_v = torch.tensor(np.array(next_states, copy = False)).to(device)\n",
        "    rewards_v = torch.tensor(rewards).to(device)\n",
        "    done_mask = torch.BoolTensor(dones).to(device)\n",
        "\n",
        "    # get Q(s, a) in vector\n",
        "    state_action_values = net(states_v).gather(1, actions_v.unsqueeze(dim = -1)).squeeze(dim = -1)\n",
        "\n",
        "    # loss between Q(s, a) - (r + GAMMA * Q'(s', a'))\n",
        "    # Q(s, a) is calculated from the current network\n",
        "    # Q'(s', a') is calculated from the target network\n",
        "    # every 1000 episodes, target network is synchronized with the current network\n",
        "    with torch.no_grad():\n",
        "      next_state_values = tgt_net(next_states_v).max(1)[0]\n",
        "      next_state_values[done_mask] = 0.0 # when it's a done-state, the value is just reward \n",
        "      next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = (GAMMA * next_state_values) + rewards_v\n",
        "\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument(\"--cuda\", default = False, action = \"store_true\", help =\"Use Cuda\")\n",
        "  parser.add_argument(\"--env\", default = DEFAULT_ENV_NAME, help = \"name of the environment\")\n",
        "\n",
        "  args = parser.parse_args()\n",
        "\n",
        "  device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "\n",
        "  env = wrappers.make_env(args.env)\n",
        "\n",
        "  net = dqn_model.DQN(env.observation_space, env.action_space.n).to(device)\n",
        "\n",
        "  # tgt_net is used to calculate Q'(s', a')\n",
        "  # this should not flow into gradient calculation\n",
        "  tgt_net = dqn_model.DQN(env.observation_space, env.action_space.n).to(device)\n",
        "\n",
        "  print(net)\n",
        "\n",
        "  buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "  agent = Agent(env, buffer)\n",
        "  epsilon = EPSILON_START\n",
        "\n",
        "  optimizer = optim.Adam(net.parameters(), lr = LEARNING_RATE)\n",
        "\n",
        "  total_rewards = []\n",
        "  frame_idx = 0\n",
        "  ts_frame = 0\n",
        "  ts = time.time()\n",
        "  best_m_reward = None\n",
        "\n",
        "  while True:\n",
        "    frame_idx += 1\n",
        "\n",
        "    # epsilon decreases linearly\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - (frame_idx / EPSILON_DECAY_LAST_FRAME))\n",
        "\n",
        "    reward = agent.play_step(net, epsilon, device = device)\n",
        "\n",
        "    if reward is not None:\n",
        "      # when an episode is finished\n",
        "      # check the progress of RL\n",
        "\n",
        "      total_rewards.append(reward)\n",
        "      speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "      ts_frame = frame_idx\n",
        "      ts = time.time()\n",
        "\n",
        "      m_reward = np.mean(total_rewards[-100:])\n",
        "\n",
        "      print(\"%d: done %d games, reward %.3f, \" \"eps %.2f, speed %.2f f/s\" % (\n",
        "                frame_idx, len(total_rewards), m_reward, epsilon,\n",
        "                speed\n",
        "            ))\n",
        "\n",
        "      if best_m_reward is None or best_m_reward < m_reward:\n",
        "        torch.save(net.state_dict(), args.env + \"-best_%.0f.dat\" % m_reward)\n",
        "\n",
        "        if best_m_reward is not None:\n",
        "          print(\"Best reward updated %.3f -> %.3f\" % (best_m_reward, m_reward))\n",
        "        \n",
        "        # update the best mean reward for the last 100 episodes\n",
        "        best_m_reward = m_reward\n",
        "\n",
        "      # the case where Pong is solved\n",
        "      if m_reward > MEAN_REWARD_BOUND:\n",
        "        print(\"Solved in %d frames!\" % frame_idx)\n",
        "        break\n",
        "\n",
        "    # wait to until enough data is collected\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "      continue \n",
        "\n",
        "    # synchronize parameters from main network to target network\n",
        "    # for every 1000 frames\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "      tgt_net.load_state_dict(net.state_dict())\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss = calc_loss(batch, net, tgt_net, device = device)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dqn_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_out(input_shape)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_out(self, shape):\n",
        "        o = self.conv(torch.zeros(1, *shape))\n",
        "        return int(np.prod(o.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
        "        return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "zv4_0kWDQNTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile wrappers.py \n",
        "\n",
        "import cv2\n",
        "import gym\n",
        "import gym.spaces\n",
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = collections.deque(maxlen=2)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "\n",
        "class ProcessFrame84(gym.ObservationWrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, obs):\n",
        "        return ProcessFrame84.process(obs)\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "        if frame.size == 210 * 160 * 3:\n",
        "            img = np.reshape(frame, [210, 160, 3]).astype(\n",
        "                np.float32)\n",
        "        elif frame.size == 250 * 160 * 3:\n",
        "            img = np.reshape(frame, [250, 160, 3]).astype(\n",
        "                np.float32)\n",
        "        else:\n",
        "            assert False, \"Unknown resolution.\"\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + \\\n",
        "              img[:, :, 2] * 0.114\n",
        "        resized_screen = cv2.resize(\n",
        "            img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        old_shape = self.observation_space.shape\n",
        "        new_shape = (old_shape[-1], old_shape[0], old_shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0, high=1.0, shape=new_shape, dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def observation(self, obs):\n",
        "        return np.array(obs).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            old_space.low.repeat(n_steps, axis=0),\n",
        "            old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(\n",
        "            self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return self.buffer\n",
        "\n",
        "\n",
        "def make_env(env_name):\n",
        "    env = gym.make(env_name)\n",
        "    env = MaxAndSkipEnv(env)\n",
        "    env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, 4)\n",
        "    return ScaledFloatFrame(env)"
      ],
      "metadata": {
        "id": "nScCJJq7QP12"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}