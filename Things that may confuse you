*** how DataLoader returns data?
e.g)
data = np.array([
    
    [[0.1, 7.4, 0],
    [-0.2, 5.3, 0],
    [0.2, 8.2, 1],
    [0.2, 7.7, 1]],

    [[1, 2, 3],
    [4, 5, 6],
    [7, 8, 9],
    [10, 11, 12]]
    ])
print(data.shape)

loader = DataLoader(data, batch_size= 1, shuffle = False)
batch = next(iter(loader))
pprint(batch)

>>>
(2, 4, 3)

tensor([[[ 0.1000,  7.4000,  0.0000],
         [-0.2000,  5.3000,  0.0000],
         [ 0.2000,  8.2000,  1.0000],
         [ 0.2000,  7.7000,  1.0000]]], dtype=torch.float64)


when the batch_size changed to 2, then
>>>
tensor([[[ 0.1000,  7.4000,  0.0000],
         [-0.2000,  5.3000,  0.0000],
         [ 0.2000,  8.2000,  1.0000],
         [ 0.2000,  7.7000,  1.0000]],

        [[ 1.0000,  2.0000,  3.0000],
         [ 4.0000,  5.0000,  6.0000],
         [ 7.0000,  8.0000,  9.0000],
         [10.0000, 11.0000, 12.0000]]], dtype=torch.float64)


============================================================================================================
*** collate_fn


============================================================================================================
*** contiguous v.s non-contiguous tensor
If the memory is contiguously stored or not

e.g)

x = torch.arange(6).view(2, 3)
print(x, x.stride())
print(x.is_contiguous())

>>>
tensor([[0, 1, 2],
        [3, 4, 5]]) (3, 1)
True

remark: the above result is correct in terms of memory image. To get the first value of the next row, it needs to move three offset.
And one move to get a next column value

Consider the following codes:
y = x.t()
print(y, y.stride())
print(y.is_contiguous())

>>>
tensor([[0, 3],
        [1, 4],
        [2, 5]]) (1, 3)
False

some operations do not work on non-contiguous tensors. Consider the following:

try:
    y = y.view(-1)
except RuntimeError as e:
    print(e)

>>> 
view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.


=================================================================================================================
*** reshape v.s view

view only works on contiguous tensors. But reshape can work with non-contiguous tensors

e.g)
x = torch.arange(4*10*2).view(4, 10, 2)
y = x.permute(2, 0, 1)

# View works on contiguous tensors
print(x.is_contiguous())
print(x.view(-1))

# Reshape works on non-contigous tensors (contiguous() + view)
print(y.is_contiguous())
try: 
    print(y.view(-1))
except RuntimeError as e:
    print(e)
print(y.reshape(-1))
print(y.contiguous().view(-1))

=================================================================================================================
*** what is leaf variable and inplcace operation

leaf variables are directly created variables. 
e.g) 
w = torch.tensor([1.0, 2.0, 3.0]) # leaf variable
x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True) # also leaf variable

y = x**2 # not a leaf variable


x += 1  # in-place
y = x + 1 # not in place

Note that PyTorch doesn’t allow in-place operations on leaf variables that have requires_grad=True 
=================================================================================================================
*** torch.Tensor v.s torch.tensor v.s torch.from_numpy

=================================================================================================================

*** from torch.utils.data import Dataset, DataLoader in PyTorch

A dataset is a Python class that allows us to get at the data we’re supplying to the neural network. 
A data loader is what feeds data from the dataset into the network

Every dataset can interact with PyTorch if it satisfies this abstract Python class:
class Dataset(object):
    def __getitem__(self, index):
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError
        
 e.g)
 class MyDataset(Dataset):
  def __init__(self, x, y):
    self.x = torch.tensor(x).float()
    self.y = torch.tensor(y).float()
  
  def __len__(self):
    return len(self.x)

  def __getitem__(self, ix):
    return self.x[ix], self.y[ix]
ds = MyDataset(X, Y)

dl = DataLoader(ds, batch_size = 2, shuffle = False) or dl = DataLoader(ds, batch_size = 2, shuffle = True)
=================================================================================================================
*** nn.Embedding(num_embedding, embedding_dim)
def: A simple lookup table that stores embeddings of a fixed dictionary and size.
this maps index to a weight matrix of certain specified dimension. And during the training, weights are adjusted to optimize the performance.
weights are initialized by randomly
e.g) 

e = nn.Embedding(10, 50) # this means that there are 10 words, and each 50-dim vector represents one word.


=================================================================================================================









